# ุฑุงูููุง ุณุฑุน - Causal Language Modeling

## ๐ ุดุฑูุน ุณุฑุน (ุจู ูุงุฑุณ)

### ูุฑุญูู 1: ูุตุจ

```bash
# ูุตุจ ูุงุจุณุชฺฏโูุง
pip install torch numpy matplotlib seaborn tqdm pandas jupyter python-docx

# ุง ุงุณุชูุงุฏู ุงุฒ requirements.txt
pip install -r requirements.txt
```

### ูุฑุญูู 2: ุชุณุช ูุตุจ

```bash
# ุงุฌุฑุง ุชุณุช
python test_setup.py
```

ุงฺฏุฑ ูพุบุงู "All tests passed! โ" ุฑุง ุฏุฏุฏุ ูุตุจ ูููู ุจูุฏู ุงุณุช.

---

## ๐ ุณุงุฎุชุงุฑ ูพุฑูฺู

```
causal-lm-project/
โโโ src/              # ฺฉุฏูุง ุงุตู
โ   โโโ model.py      # ูุฏู Transformer
โ   โโโ train.py      # ุขููุฒุด ูุฏู
โ   โโโ inference.py  # ุชููุฏ ูุชู
โ   โโโ baseline.py   # ูุฏู ูพุงู (BiGram)
โโโ demo/             # ููุชโุจูฺฉ ุขููุฒุด
โโโ docs/             # ูุณุชูุฏุงุช
โโโ models/           # ูุฏูโูุง ุฐุฎุฑูโุดุฏู
โโโ data/             # ุฏุงุฏูโูุง
โโโ README.md         # ุฑุงูููุง
```

---

## ๐ฏ ุณู ุฑูุด ุจุฑุง ุงุณุชูุงุฏู

### ุฑูุด 1: ุงุณุชูุงุฏู ุงุฒ Jupyter Notebook (ูพุดููุงุฏ!)

ุงู ุฑูุด ุจูุชุฑู ฺฏุฒูู ุจุฑุง ุงุฏฺฏุฑ ู ุขุฒูุงุด ุงุณุช:

```bash
# ุจุงุฒ ฺฉุฑุฏู ููุชโุจูฺฉ
jupyter notebook demo/demo_notebook.ipynb
```

ุฏุฑ ุงู ููุชโุจูฺฉ:
- โ ุขููุฒุด ฺฉุงูู ูุฏู
- โ ููุงุณู ุจุง Baseline
- โ ุชููุฏ ูุชู
- โ ุชุฌุณู ูุชุงุฌ
- โ ุชุญููโูุง ูุฎุชูู

ููู ฺุฒ ูุฑุญูู ุจู ูุฑุญูู ุชูุถุญ ุฏุงุฏู ุดุฏู ุงุณุช.

---

### ุฑูุด 2: ุขููุฒุด ุงุฒ ุทุฑู Command Line

ุงฺฏุฑ ูโุฎูุงูุฏ ููุท ูุฏู ุฑุง ุขููุฒุด ุฏูุฏ:

```bash
cd src
python train.py
```

ุงู ฺฉุงุฑ:
- ูุฏู ุฑุง ุขููุฒุด ูโุฏูุฏ
- Checkpointูุง ุฑุง ุฐุฎุฑู ูโฺฉูุฏ
- Tokenizer ุฑุง ูโุณุงุฒุฏ

ูุฏู ุขููุฒุดโุฏุฏู ุฏุฑ `models/best_model.pt` ุฐุฎุฑู ูโุดูุฏ.

---

### ุฑูุด 3: ุชููุฏ ูุชู (ุจุนุฏ ุงุฒ ุขููุฒุด)

```bash
# ุญุงูุช ุชุนุงูู (Interactive)
python src/inference.py --mode interactive

# ุชููุฏ ุงุฒ ฺฉ prompt
python src/inference.py --mode generate --prompt "ุงู ฺฉ ุชุณุช"

# ูุญุงุณุจู Perplexity
python src/inference.py --mode perplexity --text_file data.txt
```

---

## ๐ก ูุซุงูโูุง ุนูู

### ูุซุงู 1: ุขููุฒุด ุณุฑุน

```python
from model import CausalLanguageModel
from train import SimpleTokenizer, TextDataset, Trainer
import torch

# ุฏุงุฏูโูุง ููููู
texts = ["ุงู ฺฉ ูุชู ููููู ุงุณุช", "ูุง ุฏุฑ ุญุงู ุขููุฒุด ูุฏู ูุณุชู"] * 100

# ุณุงุฎุช Tokenizer
tokenizer = SimpleTokenizer(tokenizer_type='char')
tokenizer.build_vocab(texts)

# ุณุงุฎุช ูุฏู
model = CausalLanguageModel(
    vocab_size=len(tokenizer.vocab),
    d_model=128,
    n_layers=2,
    n_heads=4
)

# ุขููุฒุด...
```

### ูุซุงู 2: ุชููุฏ ูุชู

```python
from inference import TextGenerator

# ุจุงุฑฺฏุฐุงุฑ ูุฏู
generator = TextGenerator(
    model_path='models/best_model.pt',
    tokenizer_path='models/tokenizer.json'
)

# ุชููุฏ ูุชู
text = generator.generate(
    prompt="ููุด ูุตููุน",
    max_new_tokens=50,
    temperature=0.8
)

print(text[0])
```

---

## ๐ง ุชูุธูุงุช ูพุดููุงุฏ

### ุจุฑุง ุชุณุช ุณุฑุน:
```python
config = {
    'd_model': 128,
    'n_layers': 2,
    'n_heads': 4,
    'batch_size': 16,
    'num_epochs': 3
}
```

### ุจุฑุง ฺฉูุช ุจูุชุฑ:
```python
config = {
    'd_model': 512,
    'n_layers': 6,
    'n_heads': 8,
    'batch_size': 32,
    'num_epochs': 20
}
```

---

## โ ุณูุงูุงุช ูุชุฏุงูู

### 1. ฺุทูุฑ ุฏุงุฏู ุฎูุฏู ุฑุง ุงุณุชูุงุฏู ฺฉููุ

ุฏุงุฏูโูุง ูุชู ุฎูุฏ ุฑุง ุฏุฑ ูุงู txt ูุฑุงุฑ ุฏูุฏ:

```python
# ุฎูุงูุฏู ุฏุงุฏู
with open('my_data.txt', 'r', encoding='utf-8') as f:
    texts = f.readlines()

# ุงุฏุงูู ูุฑุงุญู...
```

### 2. ฺุทูุฑ ูุฏู ุฑุง ุจุฒุฑฺฏโุชุฑ ฺฉููุ

ุฏุฑ `train.py` ุง notebookุ ูพุงุฑุงูุชุฑูุง ุฑุง ุงูุฒุงุด ุฏูุฏ:

```python
model = CausalLanguageModel(
    vocab_size=vocab_size,
    d_model=768,      # ุจุฒุฑฺฏโุชุฑ
    n_layers=12,      # ุจุดุชุฑ
    n_heads=12,       # ุจุดุชุฑ
    d_ff=3072         # ุจุฒุฑฺฏโุชุฑ
)
```

### 3. ุขุง GPU ูุงุฒู ุงุณุชุ

- ุจุฑุง ุชุณุช ู ุงุฏฺฏุฑ: ุฎุฑุ CPU ฺฉุงู ุงุณุช
- ุจุฑุง ูุฏูโูุง ุจุฒุฑฺฏ: ุจููุ GPU ุฎู ุณุฑุนโุชุฑ ุงุณุช

ฺฉุฏ ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ GPU ุฑุง ุชุดุฎุต ูโุฏูุฏ:
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

### 4. ฺุทูุฑ ฺฉูุช ุชููุฏ ูุชู ุฑุง ุจูุจูุฏ ุฏููุ

1. ุฏุงุฏู ุจุดุชุฑ ุงุณุชูุงุฏู ฺฉูุฏ
2. ูุฏู ุฑุง ุจุฒุฑฺฏโุชุฑ ฺฉูุฏ
3. ุฒูุงู ุขููุฒุด ุฑุง ุงูุฒุงุด ุฏูุฏ
4. Temperature ุฑุง ุชูุธู ฺฉูุฏ (0.7-0.9 ูุนูููุงู ุฎูุจ ุงุณุช)
5. ุงุฒ top_k ู top_p ุงุณุชูุงุฏู ฺฉูุฏ

### 5. Perplexity ฺุณุชุ

Perplexity ูุนุงุฑ ุจุฑุง ุณูุฌุด ฺฉูุช ูุฏู ุฒุจุงู ุงุณุช:
- **ุนุฏุฏ ฺฉูฺฺฉโุชุฑ = ูุฏู ุจูุชุฑ**
- ูุซูุงู: PPL=20 ุจูุชุฑ ุงุฒ PPL=50

---

## ๐ ุฑูุน ูุดฺฉูุงุช ุฑุงุฌ

### ูุดฺฉู: Out of Memory

**ุฑุงูโุญู:**
```python
# ฺฉุงูุด batch_size
batch_size = 8  # ุจู ุฌุง 32

# ฺฉุงูุด max_seq_len
max_seq_len = 128  # ุจู ุฌุง 512

# ฺฉุงูุด ุงูุฏุงุฒู ูุฏู
d_model = 256  # ุจู ุฌุง 512
```

### ูุดฺฉู: Training ุฎู ฺฉูุฏ ุงุณุช

**ุฑุงูโุญู:**
- ุงุฒ GPU ุงุณุชูุงุฏู ฺฉูุฏ
- ุฏุงุฏู ฺฉูุชุฑ ุงุณุชูุงุฏู ฺฉูุฏ (ุจุฑุง ุชุณุช)
- ูุฏู ฺฉูฺฺฉโุชุฑ ุจุณุงุฒุฏ

### ูุดฺฉู: ูุชู ุชููุฏ ุจโูุนู ุงุณุช

**ุฑุงูโุญู:**
- ุขููุฒุด ุจุดุชุฑ (epochs ุจุดุชุฑ)
- ุฏุงุฏู ุจุดุชุฑ
- ูุฏู ุจุฒุฑฺฏโุชุฑ
- Temperature ุฑุง ฺฉุงูุด ุฏูุฏ (ูุซูุงู 0.7)

---

## ๐ ููููู ุฎุฑูุฌ

ุจุนุฏ ุงุฒ ุขููุฒุด ููููุ ฺุฒ ุดุจู ุงู ุฎูุงูุฏ ุฏุฏ:

```
Epoch 10/10
Train Loss: 1.8234 | Train PPL: 6.19
Val Loss: 2.0156 | Val PPL: 7.51

โ Model saved to models/best_model.pt
```

ู ูโุชูุงูุฏ ูุชู ุชููุฏ ฺฉูุฏ:

```
Prompt: "Machine learning"
Generated: "Machine learning is a subset of artificial 
intelligence that enables computers to learn from data..."
```

---

## ๐ ููุงุจุน ุงุฏฺฏุฑ

### ููุงูุงุช ุงุตู:
1. "Attention Is All You Need" - Transformer ุงุตู
2. "Language Models are Unsupervised Multitask Learners" - GPT-2
3. "Improving Language Understanding..." - GPT ุงููู

### ุขููุฒุดโูุง ุชูุตูโุดุฏู:
- The Illustrated Transformer (Jay Alammar)
- CS224N - Stanford NLP
- Hugging Face Course

---

## โ ฺฺฉโูุณุช ููุง

ูุจู ุงุฒ ุดุฑูุนุ ูุทูุฆู ุดูุฏ:

- [x] Python ูุตุจ ุงุณุช (3.8+)
- [x] PyTorch ูุตุจ ุงุณุช
- [x] ูุงุจุณุชฺฏโูุง ูุตุจ ุดุฏูโุงูุฏ (`pip install -r requirements.txt`)
- [x] ุชุณุช ูุตุจ ูููู ุจูุฏ (`python test_setup.py`)
- [x] ุฏุงุฏู ุขูุงุฏู ุงุณุช (ุง ุงุฒ ุฏุงุฏู ููููู ุงุณุชูุงุฏู ูโฺฉูุฏ)

ุญุงูุง ุขูุงุฏูโุงุฏ! ๐

---

## ๐ ฺฉูฺฉ ุจุดุชุฑ

ุงฺฏุฑ ูุดฺฉู ุฏุงุดุชุฏ:
1. ุงุจุชุฏุง README.md ุงุตู ุฑุง ุจุฎูุงูุฏ
2. ูุงู `docs/experiments.md` ุฑุง ุจุฑุง ุฌุฒุฆุงุช ุจุดุชุฑ ุจุจูุฏ
3. ููุชโุจูฺฉ `demo/demo_notebook.ipynb` ุฑุง ุงุฌุฑุง ฺฉูุฏ
4. ุจู Issues ุฏุฑ GitHub ูุฑุงุฌุนู ฺฉูุฏ

---

**ูููู ุจุงุดุฏ!** ๐

ุงู ูพุฑูฺู ุฑุง ุจุง โค๏ธ ุจุฑุง ุงุฏฺฏุฑ ู ุขููุฒุด ุณุงุฎุชูโุงู.
