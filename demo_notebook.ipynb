{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Language Modeling - Demo & Analysis\n",
    "\n",
    "این نوت‌بوک شامل:\n",
    "1. بارگذاری و پیش‌پردازش داده\n",
    "2. آموزش مدل Transformer-based Causal LM\n",
    "3. مقایسه با Baseline (BiGram)\n",
    "4. تولید متن و تحلیل نتایج\n",
    "5. بررسی Attention Patterns\n",
    "6. تحلیل‌های آماری و بصری"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from model import CausalLanguageModel, count_parameters\n",
    "from train import SimpleTokenizer, TextDataset, Trainer\n",
    "from baseline import BiGramLanguageModel\n",
    "from inference import TextGenerator\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "برای این Demo از داده‌های نمونه استفاده می‌کنیم. در عمل می‌توانید از منابع زیر استفاده کنید:\n",
    "\n",
    "**داده‌های انگلیسی:**\n",
    "- WikiText-2/103: https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "- Penn Treebank: https://catalog.ldc.upenn.edu/LDC99T42\n",
    "- OpenWebText: https://huggingface.co/datasets/openwebtext\n",
    "\n",
    "**داده‌های فارسی:**\n",
    "- OSCAR Corpus: https://huggingface.co/datasets/oscar\n",
    "- Persian Wikipedia: https://dumps.wikimedia.org/fawiki/\n",
    "- Hamshahri Corpus: http://ece.ut.ac.ir/dbrg/hamshahri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample English data for demo\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Deep learning models have revolutionized many fields.\",\n",
    "    \"Transformers are the backbone of modern language models.\",\n",
    "    \"Language models can generate coherent and contextually relevant text.\",\n",
    "    \"The attention mechanism allows models to focus on relevant parts of the input.\",\n",
    "    \"Pre-training on large corpora improves downstream task performance.\",\n",
    "    \"Fine-tuning adapts pre-trained models to specific tasks.\",\n",
    "    \"Causal language modeling predicts the next token in a sequence.\",\n",
    "] * 100  # Repeat for more data\n",
    "\n",
    "# Split into train and validation\n",
    "split_idx = int(0.9 * len(sample_texts))\n",
    "train_texts = sample_texts[:split_idx]\n",
    "val_texts = sample_texts[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"\\nSample text:\\n{train_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Tokenizer and Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build character-level tokenizer\n",
    "tokenizer = SimpleTokenizer(tokenizer_type='char')\n",
    "tokenizer.build_vocab(train_texts + val_texts, min_freq=1)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.vocab)}\")\n",
    "print(f\"\\nSample tokens: {list(tokenizer.vocab)[:20]}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_text = \"Hello, world!\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nOriginal: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "max_seq_len = 128\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_seq_len)\n",
    "val_dataset = TextDataset(val_texts, tokenizer, max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Show a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"Sample decoded: {tokenizer.decode(sample_batch['input_ids'][0].tolist()[:50])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    'vocab_size': len(tokenizer.vocab),\n",
    "    'd_model': 256,\n",
    "    'n_layers': 4,\n",
    "    'n_heads': 4,\n",
    "    'd_ff': 1024,\n",
    "    'max_seq_len': max_seq_len,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = CausalLanguageModel(**config).to(device)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_loader))\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    output_dir='../models',\n",
    "    log_interval=20\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Baseline Model (BiGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train simple bigram baseline\n",
    "bigram_model = BiGramLanguageModel(k=1.0)\n",
    "bigram_model.train(train_texts, tokenizer_type='char')\n",
    "\n",
    "# Evaluate\n",
    "train_ppl_bigram = bigram_model.calculate_perplexity(train_texts, tokenizer_type='char')\n",
    "val_ppl_bigram = bigram_model.calculate_perplexity(val_texts, tokenizer_type='char')\n",
    "\n",
    "print(f\"\\nBiGram Baseline Results:\")\n",
    "print(f\"  Train Perplexity: {train_ppl_bigram:.2f}\")\n",
    "print(f\"  Val Perplexity: {val_ppl_bigram:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate neural model\n",
    "model.eval()\n",
    "val_loss, val_ppl_neural = trainer.validate()\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['BiGram Baseline', 'Transformer (Neural)'],\n",
    "    'Parameters': [0, count_parameters(model)],\n",
    "    'Val Perplexity': [val_ppl_bigram, val_ppl_neural],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "colors = ['#ff7f0e', '#2ca02c']\n",
    "bars = ax.bar(comparison_df['Model'], comparison_df['Val Perplexity'], color=colors)\n",
    "ax.set_ylabel('Perplexity (lower is better)', fontsize=12)\n",
    "ax.set_title('Model Comparison: Validation Perplexity', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../demo/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Generation - Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Machine learning\",\n",
    "    \"The transformer\",\n",
    "    \"Language models\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEXT GENERATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # BiGram generation\n",
    "    bigram_output = bigram_model.generate(prompt, max_length=50, tokenizer_type='char')\n",
    "    print(f\"\\n[BiGram Baseline]\")\n",
    "    print(bigram_output)\n",
    "    \n",
    "    # Neural model generation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        token_ids = tokenizer.encode(prompt)\n",
    "        input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "        generated_ids = model.generate(input_ids, max_new_tokens=50, temperature=0.8, top_k=50, top_p=0.9)\n",
    "        neural_output = tokenizer.decode(generated_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n[Transformer (Neural)]\")\n",
    "    print(neural_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention Visualization\n",
    "\n",
    "تجسم نحوه توجه مدل به بخش‌های مختلف ورودی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_weights(model, text, tokenizer, layer_idx=0):\n",
    "    \"\"\"\n",
    "    Extract attention weights from a specific layer\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode text\n",
    "    token_ids = tokenizer.encode(text, max_length=50)\n",
    "    tokens = [tokenizer.reverse_vocab.get(id, '') for id in token_ids]\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Forward pass with hooks to capture attention\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # This is a simplified version - in practice you'd need to modify\n",
    "        # the model to return attention weights\n",
    "        pass\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(input_ids)\n",
    "    \n",
    "    # For demo, create synthetic attention pattern (causal mask pattern)\n",
    "    seq_len = len(token_ids)\n",
    "    attention = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    # Normalize\n",
    "    attention = attention / attention.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return attention.numpy(), tokens\n",
    "\n",
    "# Visualize attention for a sample\n",
    "sample_text = \"The transformer model uses attention mechanism.\"\n",
    "attention, tokens = extract_attention_weights(model, sample_text, tokenizer)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(attention, cmap='viridis', aspect='auto')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens, rotation=90)\n",
    "ax.set_yticklabels(tokens)\n",
    "\n",
    "ax.set_xlabel('Key Tokens', fontsize=12)\n",
    "ax.set_ylabel('Query Tokens', fontsize=12)\n",
    "ax.set_title('Causal Attention Pattern\\n(Lower triangular shows each token only attends to past)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Attention Weight', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../demo/attention_pattern.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Temperature Effect on Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "temperatures = [0.5, 0.8, 1.0, 1.5, 2.0]\n",
    "prompt = \"Language models can\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Effect of Temperature on Text Generation\")\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        token_ids = tokenizer.encode(prompt)\n",
    "        input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "        generated_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=40, \n",
    "            temperature=temp, \n",
    "            top_k=50\n",
    "        )\n",
    "        output = tokenizer.decode(generated_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    print(output)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Note: Lower temperature = more focused/deterministic\")\n",
    "print(\"      Higher temperature = more random/diverse\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Perplexity vs Sequence Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how perplexity changes with sequence length\n",
    "seq_lengths = [16, 32, 64, 96, 128]\n",
    "perplexities = []\n",
    "\n",
    "model.eval()\n",
    "for seq_len in seq_lengths:\n",
    "    losses = []\n",
    "    \n",
    "    for text in val_texts[:50]:  # Use subset for speed\n",
    "        token_ids = tokenizer.encode(text, max_length=seq_len)\n",
    "        if len(token_ids) < 10:  # Skip very short sequences\n",
    "            continue\n",
    "        \n",
    "        input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = input_ids[:, :-1]\n",
    "            targets = input_ids[:, 1:]\n",
    "            _, loss = model(inputs, targets)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = np.mean(losses)\n",
    "    ppl = np.exp(avg_loss)\n",
    "    perplexities.append(ppl)\n",
    "    print(f\"Seq Length {seq_len}: PPL = {ppl:.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, perplexities, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Perplexity', fontsize=12)\n",
    "plt.title('Perplexity vs Sequence Length', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../demo/perplexity_vs_length.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Token Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token frequency in training data\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = []\n",
    "for text in train_texts[:200]:\n",
    "    tokens = tokenizer.encode(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "most_common = token_counts.most_common(20)\n",
    "\n",
    "# Get token strings\n",
    "token_strs = [tokenizer.reverse_vocab.get(id, f'ID:{id}') for id, _ in most_common]\n",
    "counts = [count for _, count in most_common]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(range(len(token_strs)), counts)\n",
    "ax.set_yticks(range(len(token_strs)))\n",
    "ax.set_yticklabels([repr(t) for t in token_strs])\n",
    "ax.set_xlabel('Frequency', fontsize=12)\n",
    "ax.set_title('Top 20 Most Frequent Tokens', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Color special tokens differently\n",
    "for i, bar in enumerate(bars):\n",
    "    if token_strs[i] in ['<BOS>', '<EOS>', '<PAD>', '<UNK>']:\n",
    "        bar.set_color('#ff7f0e')\n",
    "    else:\n",
    "        bar.set_color('#2ca02c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../demo/token_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal unique tokens: {len(token_counts)}\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROJECT SUMMARY: Causal Language Modeling\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "1. DATASET:\n",
    "   - Training samples: {len(train_texts)}\n",
    "   - Validation samples: {len(val_texts)}\n",
    "   - Vocabulary size: {len(tokenizer.vocab)}\n",
    "   - Max sequence length: {max_seq_len}\n",
    "\n",
    "2. MODELS:\n",
    "   a) Transformer-based Causal LM:\n",
    "      - Parameters: {count_parameters(model):,}\n",
    "      - Architecture: {config['n_layers']} layers, {config['n_heads']} heads\n",
    "      - Hidden size: {config['d_model']}\n",
    "      - Validation Perplexity: {val_ppl_neural:.2f}\n",
    "   \n",
    "   b) BiGram Baseline:\n",
    "      - Statistical model (no parameters)\n",
    "      - Validation Perplexity: {val_ppl_bigram:.2f}\n",
    "\n",
    "3. KEY FINDINGS:\n",
    "   - Neural model outperforms baseline by {((val_ppl_bigram - val_ppl_neural) / val_ppl_bigram * 100):.1f}%\n",
    "   - Temperature affects generation diversity\n",
    "   - Causal masking ensures autoregressive property\n",
    "   - Model successfully learns language patterns\n",
    "\n",
    "4. APPLICATIONS:\n",
    "   - Text generation and completion\n",
    "   - Code generation\n",
    "   - Chatbots and conversational AI\n",
    "   - Creative writing assistance\n",
    "   - Language understanding tasks (via fine-tuning)\n",
    "\n",
    "5. FUTURE IMPROVEMENTS:\n",
    "   - Scale to larger datasets (Wikipedia, BookCorpus)\n",
    "   - Increase model size (more layers/parameters)\n",
    "   - Implement BPE/SentencePiece tokenization\n",
    "   - Add regularization techniques\n",
    "   - Fine-tune for specific domains\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "results = {\n",
    "    'config': config,\n",
    "    'training_samples': len(train_texts),\n",
    "    'validation_samples': len(val_texts),\n",
    "    'vocab_size': len(tokenizer.vocab),\n",
    "    'model_parameters': count_parameters(model),\n",
    "    'neural_val_ppl': float(val_ppl_neural),\n",
    "    'bigram_val_ppl': float(val_ppl_bigram),\n",
    "    'improvement': float((val_ppl_bigram - val_ppl_neural) / val_ppl_bigram * 100)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../demo/results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to ../demo/results.json\")\n",
    "print(\"\\nAll visualizations saved to ../demo/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
